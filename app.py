import streamlit as st
from supabase import create_client
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import io
import plotly.express as px
import numpy as np
import itertools

from portfolio_upload import load_portfolio_from_csv
from returns_analysis import compute_returns, normalize_prices
from price_display import display_raw_price_data, display_normalized_price_data, offer_price_data_download

from correlation_analysis import (
    compute_pairwise_correlation,
    flatten_correlation_matrix,
    get_top_correlations,
    plot_correlation_heatmap
)

from rolling_correlation import display_rolling_correlation_viewer

from risk_analysis import (
    get_risk_metrics,
    compute_volatility_table,
    compute_portfolio_risk,
    compute_benchmark_metrics,
    optimize_portfolio,
    suggest_portfolio_tweaks,
)

from covariance_analysis import compute_covariance_matrix, plot_covariance_heatmap

from antifragility_analysis import compute_antifragility_scores, display_antifragility_table

import datetime

from risk_display import render_concentration_metrics, show_benchmark_metrics

def render_concentration_metrics(portfolio_weights):
    from risk_analysis import compute_concentration_risk

    hhi, hhi_norm = compute_concentration_risk(portfolio_weights.values)
    st.metric("Herfindahl Index (HHI)", f"{hhi:.4f}")
    st.metric("Normalized HHI", f"{hhi_norm * 100:.2f}%")

def render_results(df, returns, df_norm, tickers, start, end, portfolio_weights):
    returns_clean = returns.replace([np.inf, -np.inf], np.nan).dropna()

    tab_labels = [
    "üìä Prices",
    "üìà Correlation Matrix",
    "üìâ Covariance Matrix",
    "üìâ Volatility Overview",
    "üßÆ Risk Metrics (Asset-Level)",
    "üìê Sharpe & Sortino (Portfolio)",
    "‚öôÔ∏è Optimization & Suggestions",
    "üìä Cumulative Returns",
    "üìâ Alpha/Beta/Tracking Error",
    "üß¨ Antifragility"
    ]

    tabs = st.tabs(tab_labels)
    scaling = np.sqrt(252) # Annualization factor for daily returns
    
    # Price Visuals
    with tabs[0]:
        display_raw_price_data(df)
        offer_price_data_download(df)
        display_normalized_price_data(df_norm)

    # Correlation Matrix
    with tabs[1]:
        st.subheader("üìç Key Correlation Highlights")
        corr_type = st.session_state.get("corr_type", "Pearson")
        corr = compute_pairwise_correlation(returns, method=corr_type)
        st.session_state["corr"] = corr
        corr_flat = flatten_correlation_matrix(corr)
        top_pos, top_neg, strong_corr = get_top_correlations(corr_flat)

        st.markdown("### üîù Top 5 Positively Correlated Pairs")
        st.dataframe(top_pos.reset_index(drop=True).round(3))

        st.markdown("### üîª Top 5 Negatively Correlated Pairs")
        st.dataframe(top_neg.reset_index(drop=True).round(3))

        st.markdown("### ‚ö†Ô∏è Pairs with |Correlation| > 0.8")
        st.dataframe(strong_corr.reset_index(drop=True).round(3))

        st.subheader(f"üìå {corr_type} Correlation Matrix")
        fig = plot_correlation_heatmap(corr, corr_type)
        st.plotly_chart(fig, use_container_width=True, key="correlation_heatmap_main")

        # Cross-Sector Correlation (TWSC-style)
        if "portfolio_classifications" in st.session_state:
            class_map = st.session_state["portfolio_classifications"]
            if class_map is not None:
                st.subheader("üß† Thematic / Sector Correlation Analysis (TWSC)")
                tickers_with_class = [t for t in returns.columns if t in class_map.index]
                returns_classified = returns[tickers_with_class]
                class_map_filtered = class_map.loc[tickers_with_class]

                corr = compute_pairwise_correlation(returns_classified)
                corr_values = corr.stack().reset_index()
                corr_values.columns = ["Ticker A", "Ticker B", "Correlation"]
                corr_values["Class A"] = corr_values["Ticker A"].map(class_map_filtered)
                corr_values["Class B"] = corr_values["Ticker B"].map(class_map_filtered)
                corr_values = corr_values[corr_values["Ticker A"] != corr_values["Ticker B"]]

                grouped_corr = corr_values.groupby(["Class A", "Class B"])["Correlation"].mean().unstack().round(2)
                st.markdown("### üßæ Average Correlation Between Groups")
                st.dataframe(grouped_corr.fillna("-"))

                try:
                    fig = px.imshow(
                        grouped_corr,
                        text_auto=True,
                        title="Cross-Group Correlation Heatmap (TWSC)",
                        color_continuous_scale="RdBu_r"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.warning(f"Failed to render sector heatmap: {e}")

        display_rolling_correlation_viewer(returns, tickers)

    # Covariance Matrix
    with tabs[2]:
        st.subheader("üìâ Covariance Matrix")
        with st.expander("‚ùì What Is Covariance and How to Use It?"):
            st.markdown(r"""
        ### üîó Covariance Matrix

        Covariance measures **how two assets move together**.

        ---

        ### üßÆ Formula:
        $$
        \text{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)]
        $$

        - If positive ‚Üí assets rise/fall together  
        - If negative ‚Üí one rises while the other falls  
        - Units: %¬≤ ‚Üí hard to compare across assets

        ---

        ### üß† Use It To:
        - Build portfolios where assets don‚Äôt ‚Äúmove in sync‚Äù
        - Feed into optimization models (like Markowitz MV)

        ‚ö†Ô∏è Covariance is **scale-sensitive** ‚Üí use correlation for normalized comparison
        """)
        try:
            cov_matrix = compute_covariance_matrix(returns)
            st.dataframe(cov_matrix.round(4))
            buffer_cov = io.StringIO()
            cov_matrix.to_csv(buffer_cov)
            st.download_button("‚¨áÔ∏è Download Covariance Matrix", buffer_cov.getvalue(), "covariance_matrix.csv", "text/csv")
            plot_covariance_heatmap(cov_matrix)
        except Exception as e:
            st.warning(f"Failed to compute covariance matrix: {e}")

    # Volatility
    with tabs[3]:
        from risk_display import (
            render_volatility_tables,
            render_relative_volatility_table,
            render_rolling_volatility,
            render_tracking_error,
            render_longitudinal_volatility_table,
            render_garch_volatility_forecast
        )
        st.subheader("üìâ Volatility Overview")
        render_volatility_tables(returns_clean)
        render_relative_volatility_table(returns_clean)
        render_rolling_volatility(returns_clean, portfolio_weights)
        render_longitudinal_volatility_table(returns_clean)
        render_garch_volatility_forecast(returns_clean)

    # Risk Metrics
    with tabs[4]:
        from risk_display import render_risk_metrics, render_return_histogram
        render_risk_metrics(returns_clean, scaling)
        with st.expander("‚ùì What Does the Return Histogram Show?"):
            st.markdown(r"""
        ### üìä Portfolio Return Histogram

        This shows the **distribution of your daily portfolio returns**.

        - Taller bars = more frequent returns in that range  
        - Left-skewed = more negative returns  
        - Right-skewed = more positive returns

        üß† **Use it to**:
        - See if your returns are lopsided
        - Validate assumptions for Sharpe/Sortino
        """)
        render_return_histogram(returns_clean, portfolio_weights)

    # Sharpe & Sortino Ratios
    with tabs[5]:
        st.subheader("üìä Sharpe & Sortino Ratios")

        st.warning("‚ö†Ô∏è These Sharpe and Sortino values are currently **inaccurate** as daily prices are used instead of end-of-month NAVs or a dynamic risk-free rate.")

        with st.expander("‚ùì Why Might These Numbers Be Inaccurate?"):
            st.markdown(r"""
        ### ‚ö†Ô∏è Accuracy Warning

        Sharpe and Sortino require:
        - Proper **end-of-month portfolio returns**
        - Accurate **risk-free rate over time**
        - Consistent **weight alignment**

        üìâ If you're uploading only raw tickers + shares:
        - Daily returns may not match your actual portfolio behavior
        - Static risk-free rate (0.02/252) may be unrealistic

        ‚úÖ For best accuracy: upload NAV-based return series.
        """)

        # ‚ÑπÔ∏è Explanatory Section
        with st.expander("‚ùì What Does the Sharpe Ratio Actually Mean?"):
            st.markdown(r"""
        ### üìà Sharpe Ratio ‚Äì What It Means

        The **Sharpe Ratio** answers:

        > ‚ÄúHow much extra return am I earning for each unit of risk I take?‚Äù

        It shows how efficiently a portfolio converts **volatility into profit**, after removing the **risk-free return** (like T-bills).

        ---

        ### üßÆ Formula:
        $$
        \text{Sharpe Ratio} = \frac{ \mathbb{E}[R_p - R_f] }{ \sigma_p } \times \sqrt{252}
        $$

        Where:
        - \( R_p \): Portfolio return  
        - \( R_f \): Risk-free rate  
        - \( \sigma_p \): Standard deviation of portfolio returns  
        - \( \sqrt{252} \): Annualization factor

        ---

        ### üß† Interpretation:

        | Sharpe Ratio | What It Means |
        |--------------|----------------|
        | > 2.0        | ‚úÖ Excellent risk-adjusted return |
        | 1.0 ‚Äì 2.0    | üëç Good balance of return vs. risk |
        | < 1.0        | ‚ö†Ô∏è Too much risk for the return gained |

        ---

        ### üõ†Ô∏è How to Use It:
        - Compare funds: **higher Sharpe = better** use of risk.
        - Evaluate trade-offs: Is this strategy giving enough return for the volatility?

        ---

        ### ‚ö†Ô∏è Caution:
        Sharpe penalizes all volatility ‚Äî even **upside**.  
        If you care more about protecting the downside, use **Sortino Ratio** instead.
        """)
            
        with st.expander("‚ùì What Does the Sortino Ratio Mean?"):
            st.markdown(r"""
        ### üéØ Sortino Ratio ‚Äì Focused on Downside Risk

        The **Sortino Ratio** answers:

        > ‚ÄúHow much excess return am I earning **per unit of downside risk**?‚Äù

        Unlike Sharpe, it **ignores good (upside) volatility** and only penalizes losses.

        ---

        ### üßÆ Formula:
        $$
        \text{Sortino Ratio} = \frac{ \mathbb{E}[R_p - R_f] }{ \sigma_{\text{down}} } \times \sqrt{252}
        $$

        Where:
        - \( R_p \): Portfolio return  
        - \( R_f \): Risk-free rate  
        - \( \sigma_{\text{down}} \): Std dev of negative excess returns only

        ---

        ### üß† Interpretation:

        | Sortino Ratio | What It Means |
        |---------------|----------------|
        | > 2.0         | ‚úÖ Excellent downside protection |
        | 1.0‚Äì2.0       | üëç Decent return vs. downside |
        | < 1.0         | ‚ö†Ô∏è Too risky for the return gained |

        ---

        ### üõ†Ô∏è How to Use It:
        - Great for conservative or retirement-focused portfolios  
        - Compare with Sharpe:  
        - **Sharpe high + Sortino low** = returns driven by big swings (not ideal)
        """)


        if portfolio_weights is not None:
            st.download_button("üì• Export returns_clean (aligned)", returns_clean[portfolio_weights.index].to_csv(), "returns_clean_aligned.csv")
            st.download_button("üì• Export portfolio_weights", portfolio_weights.to_csv(), "weights.csv")

        # Actual metric calculation
        scaling_annual = np.sqrt(252)
        scaling_monthly = np.sqrt(12)

        def compute_ratios(returns, scale, risk_free_rate=0.02 / 252):
            excess_returns = returns - risk_free_rate
            mean_excess = excess_returns.mean()
            std_total = returns.std()
            std_downside = excess_returns[excess_returns < 0].std()
            sharpe = mean_excess / std_total * scale if std_total > 0 else np.nan
            sortino = mean_excess / std_downside * scale if std_downside > 0 else np.nan
            return sharpe, sortino

        if portfolio_weights is not None:
            uploaded_returns = returns_clean.dot(portfolio_weights)
            sharpe_ann, sortino_ann = compute_ratios(uploaded_returns, scaling_annual)
            sharpe_month, sortino_month = compute_ratios(uploaded_returns, scaling_monthly)

            st.markdown("### üìà Uploaded Portfolio")
            st.metric("Sharpe (Annualized)", f"{sharpe_ann:.4f}")
            st.metric("Sortino (Annualized)", f"{sortino_ann:.4f}")
            st.metric("Sharpe (Monthly)", f"{sharpe_month:.4f}")
            st.metric("Sortino (Monthly)", f"{sortino_month:.4f}")
        else:
            st.info("üìÇ No uploaded portfolio weights provided. Showing optimized portfolio metrics.")
            opt_weights = optimize_portfolio(returns_clean).values.flatten()
            opt_returns = returns_clean.dot(opt_weights)
            sharpe_ann, sortino_ann = compute_ratios(opt_returns, scaling_annual)
            sharpe_month, sortino_month = compute_ratios(opt_returns, scaling_monthly)

            st.markdown("### ‚öôÔ∏è Optimized Portfolio")
            st.metric("Sharpe (Annualized)", f"{sharpe_ann:.4f}")
            st.metric("Sortino (Annualized)", f"{sortino_ann:.4f}")
            st.metric("Sharpe (Monthly)", f"{sharpe_month:.4f}")
            st.metric("Sortino (Monthly)", f"{sortino_month:.4f}")

    # Optimization & Suggestions
    with tabs[6]:
        from risk_display import render_portfolio_optimization, render_concentration_metrics
        st.subheader("‚öôÔ∏è Optimization & Risk Suggestions")
        st.caption("Generated by analyzing marginal volatility and downside risk.")
        if portfolio_weights is not None:
            st.subheader("üìä Concentration Risk (HHI)")
            with st.expander("‚ùì What Is Concentration Risk and HHI?"):
                st.markdown(r"""
            ### ‚öñÔ∏è Herfindahl-Hirschman Index (HHI)

            Measures how concentrated your portfolio is in a few names.

            ---

            ### üßÆ Formula:
            $$
            HHI = \sum_{i=1}^{n} w_i^2
            $$

            - \( w_i \) = portfolio weight of asset \( i \)  
            - Range:  
            - 1/n = perfectly diversified  
            - 1 = all in one stock

            ‚úÖ Use **Normalized HHI** to scale between 0 (diversified) and 1 (concentrated)
            """)
            render_concentration_metrics(portfolio_weights)
            suggestions = suggest_portfolio_tweaks(portfolio_weights, returns_clean)
            st.markdown("### üí¨ Suggestions")
            with st.expander("‚ùì How Are These Suggestions Generated?"):
                st.markdown(r"""
            ### üß† Portfolio Improvement Suggestions

            We analyze:
            - **Marginal Risk Contribution (MRC)**
            - **Volatility**
            - **Sharpe**
            - **Beta**
            - **VaR**
            - **Concentration**

            üß† Suggestions are based on:
            - Reallocating from **overly risky** to **under-risked** assets
            - Lowering exposure to high-volatility or low-Sharpe assets
            - Balancing risk contributions (not just weights)

            ‚úÖ All suggestions **preserve your asset list** ‚Äî only weight tweaks.
            """)
            for s in suggestions:
                st.markdown(f"- {s}")
        else:
            st.info("üìÇ No uploaded portfolio weights available for concentration analysis.")

    # Cumulative Returns
    with tabs[7]:
        from risk_display import render_return_visuals
        with st.expander("‚ùì What Is Cumulative Return?"):
            st.markdown(r"""
        ### üìä Cumulative Return = Total Growth Over Time

        It shows:
        - How much your portfolio (or each strategy) grew over time
        - Compounded from daily returns:  
        $$\text{Cumulative} = \prod_{t=1}^{T} (1 + r_t)$$

        üß† **Use it to**:
        - Compare long-term growth
        - Spot performance divergence from optimized portfolios
        """)
        render_return_visuals(returns_clean, portfolio_weights, scaling)
    
    # Benchmark
    with tabs[8]:
        from risk_display import render_tracking_error
        st.subheader("üìâ Alpha/Beta/Correlation vs Benchmark")
        st.warning("‚ö†Ô∏è Alpha/Beta metrics may be inaccurate or misleading depending on how your portfolio data is structured.")

        with st.expander("‚ùì Why Might These Metrics Be Inaccurate?"):
            st.markdown(r"""
        ### ‚ö†Ô∏è Alpha & Beta Accuracy Warning

        Alpha and Beta are powerful ‚Äî but only when based on **accurate, time-aligned return data**.

        ---

        #### üß≠ What They Require:
        - Your portfolio return stream **must match the benchmark's frequency** (typically **weekly**)
        - Enough **overlapping dates** to run a regression
        - A **benchmark that actually reflects your investment universe**

        ---

        ### üîç Common Accuracy Issues:

        ---

        #### 1Ô∏è‚É£ Misaligned Return Frequency

        The app uses the code:
        ```python
        portfolio_returns.resample("W-FRI").mean()
        benchmark_returns.resample("W-FRI").last().pct_change()
        ```
        
        This assumes your portfolio is **rebalanced weekly**, which may not reflect reality.

        > If you're using daily prices with static weights, averaging returns to weekly can produce distorted results.

        ---

        #### 2Ô∏è‚É£ Sparse or Missing Overlap

        If your portfolio return series doesn't align closely in time with the benchmark:
        - The regression (for Beta) becomes unstable
        - The resulting **Alpha** value may be meaningless

        > For example: Alpha over 3 data points = statistically unreliable.

        ---

        #### 3Ô∏è‚É£ Benchmark Doesn‚Äôt Match Strategy

        Even if your portfolio holds:
        - Global equities
        - Thematic ETFs
        - Crypto-related assets

        You may still be comparing to:
        - S&P 500 (^GSPC)
        - Nasdaq-100 (^NDX)

        > This leads to **misleading Alpha/Beta values** because the benchmark isn't representative.

        ---

        ### ‚úÖ Best Practices

        | Issue | Fix |
        |-------|-----|
        | Frequency mismatch | Use returns sampled at the same frequency |
        | Sparse data | Ensure full date alignment between portfolio & benchmark |
        | Benchmark mismatch | Select a more relevant benchmark or construct your own |

        ---

        üß† **Tip:** Upload NAV-based weekly returns if available ‚Äî that gives the cleanest Alpha/Beta estimates.
        """)


        with st.expander("‚ùì Alpha, Beta, and Tracking Error Explained"):
            st.markdown(r"""
        ### üìâ Benchmark Risk Metrics

        ---

        #### üèÜ Alpha
        Measures **excess return** vs. a benchmark.

        **Formula:**
        $$
        \alpha = R_p - \beta \cdot R_b
        $$

        - $R_p$: Portfolio return  
        - $R_b$: Benchmark return  
        - $\beta$: Sensitivity to market

        ‚úÖ **Higher alpha = outperformance**

        ---

        #### üìà Beta
        Measures **sensitivity to market moves**.

        **Formula:**
        $$
        \beta = \frac{\text{Cov}(R_p, R_b)}{\text{Var}(R_b)}
        $$

        - $\beta = 1$: Matches market  
        - $\beta > 1$: More volatile  
        - $\beta < 1$: Less volatile

        ---

        #### üìè Tracking Error
        Measures how closely your portfolio **tracks** the benchmark.

        **Formula:**
        $$
        \text{TE} = \text{std}(R_p - R_b) \times \sqrt{252}
        $$

        ‚úÖ **Lower = closer match**, higher = more active bets
        """)
        st.markdown("""
        ---

        ### üõ†Ô∏è How to Use These Metrics

        | Metric | Meaning | Goal |
        |--------|---------|------|
        | Alpha | Excess return vs benchmark | ‚Üë Higher |
        | Beta | Sensitivity to market moves | ~1 for passive, <1 for low risk |
        | Tracking Error | Deviation from benchmark | ‚Üì Lower for passive, ‚Üë Higher for active |
        """)
        st.caption("üìò Alpha and Beta are calculated using weekly (Friday-to-Friday) returns over the past 3 months.")

        default_benchmarks = {
            "S&P 500": "^GSPC",
            "Nasdaq-100": "^NDX",
            "Dow Jones": "^DJI",
            "STI": "^STI"
        }

        user_input = st.sidebar.text_input("‚ûï Add Benchmarks (format: ^DJI:Dow, ^STI:STI)", key="benchmark_input")
        user_benchmarks = {}
        if user_input:
            for entry in user_input.split(","):
                if ":" in entry:
                    ticker, label = entry.split(":")
                    user_benchmarks[label.strip()] = ticker.strip()

        benchmarks = {**default_benchmarks, **user_benchmarks}
        selected_benchmark_label = st.selectbox("üìà Benchmark for Tracking Error", list(benchmarks.keys()))
        benchmark_symbol = benchmarks[selected_benchmark_label]

        st.subheader("üìè Tracking Error")

        with st.expander("‚ùì What is Tracking Error?"):
            st.markdown("""
            ### üìè Tracking Error

            Measures how closely your portfolio tracks a benchmark.

            $$
            \text{TE} = \text{std}(R_p - R_b) \times \sqrt{252}
            $$

            - $R_p$: Portfolio return  
            - $R_b$: Benchmark return

            **Low TE** ‚Üí tightly follows benchmark  
            **High TE** ‚Üí deviates from benchmark  
            """)

        render_tracking_error(returns_clean, portfolio_weights, benchmark_symbol)

        st.markdown("---")
        
        if portfolio_weights is not None:
            portfolio_daily = returns_clean.dot(portfolio_weights)

            if portfolio_daily.empty:
                st.error("‚ùå Portfolio returns are empty!")
            else:
                benchmark_results = show_benchmark_metrics(portfolio_daily, benchmarks, start, end)
                if benchmark_results:
                    df_benchmark = pd.DataFrame(benchmark_results)

                    # Handle edge case: if somehow Alpha/Beta/Correlation are NaN
                    if df_benchmark.empty or df_benchmark[["Alpha", "Beta", "Correlation"]].isnull().all().all():
                        st.warning("‚ö†Ô∏è Computed benchmark results are empty or contain only NaNs.")
                    else:
                        st.dataframe(df_benchmark.round(4))
                        best = max(benchmark_results, key=lambda x: x["Alpha"])
                        st.success(f"üèÜ Top Benchmark Outperformance: {best['Benchmark']} with Alpha = {best['Alpha']:.2%}")
                else:
                    st.warning("‚ö†Ô∏è No benchmark metrics calculated. Possibly no overlapping data.")

    # Antifragility Analysis
    with tabs[9]:
        st.subheader("üß¨ Antifragility Analysis")
        with st.expander("‚ùì What Is Antifragility Score?"):
            st.markdown(r"""
        ### üß¨ Antifragility = Resilience During Market Stress

        This score rewards:
        - Assets that **rise when the rest fall**
        - Or at least **fall less** during downturns

        ---

        ### üßÆ Formula:
        $$
        \text{Antifragility Score} = - (\text{Correlation to Portfolio}) \times (\text{Downside Capture})
        $$

        - **Downside capture** = how much of portfolio loss the asset absorbs  
        $$ \text{DSC} = \frac{\text{Asset Return (when Portfolio < 0)}}{\text{Portfolio Return (when Portfolio < 0)}} $$

        ---

        ### üß† How to Use It:
        - Rank assets by resilience
        - Identify good hedges or diversifiers
        """)
        try:
            scores_df = compute_antifragility_scores(returns)
            st.caption("üîç Antifragility Score = - (correlation √ó downside capture). Higher = more resilient to drawdowns.")
            st.dataframe(scores_df.style.highlight_max(axis=0).format("{:.2f}"))

            buffer_anti = io.StringIO()
            scores_df.to_csv(buffer_anti)
            st.download_button("‚¨áÔ∏è Download Antifragility Scores", buffer_anti.getvalue(), "antifragility_scores.csv", "text/csv")
        except Exception as e:
            st.warning(f"Failed to compute antifragility scores: {e}")
                        
# Securely load from .streamlit/secrets.toml
SUPABASE_URL = st.secrets["supabase"]["url"]
SUPABASE_KEY = st.secrets["supabase"]["key"]
# supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def get_client():
    client = create_client(SUPABASE_URL, SUPABASE_KEY)
    user = st.session_state.get("user")
    if user:
        client.auth.set_session(user["access_token"], user["refresh_token"])
    return client

supabase = get_client()

try:
    import riskfolio as rp
    riskfolio_available = True
except ImportError:
    riskfolio_available = False

st.set_page_config(page_title="Correlation & Risk Dashboard", layout="wide")
st.title("üìà Dynamic Stock Correlation & Risk Analysis")

# ‚úÖ Show toast if group was just saved
if "last_saved_group" in st.session_state:
    st.success(f"‚úÖ Group '{st.session_state['last_saved_group']}' saved successfully!")
    del st.session_state["last_saved_group"]

# ---------------------------------------------
# Sidebar Configuration Inputs
# ---------------------------------------------

st.sidebar.header("Configuration")
st.sidebar.title("üîê Login")

auth_action = st.sidebar.radio("Choose:", ["Login", "Signup"])
email = st.sidebar.text_input("Email")
password = st.sidebar.text_input("Password", type="password")

if st.sidebar.button(auth_action):
    try:
        if auth_action == "Signup":
            user = supabase.auth.sign_up({"email": email, "password": password})
            st.sidebar.success("‚úÖ Please check your email to confirm your account. After confirming, you can log in.")
        else:
            auth_result = supabase.auth.sign_in_with_password({
                "email": email,
                "password": password
            })

            session = supabase.auth.get_session()
            if session:
                if auth_result.user.confirmed_at is None:
                    st.sidebar.warning("üìß Please confirm your email before logging in.")
                else:
                    st.session_state["user"] = {
                        "id": auth_result.user.id,
                        "email": auth_result.user.email,
                        "access_token": session.access_token,
                        "refresh_token": session.refresh_token
                    }
                    st.sidebar.success("‚úÖ Logged in!")
                    st.rerun()
            else:
                st.sidebar.error("‚ùå Login failed: No valid session.")

    except Exception as e:
        st.sidebar.error(f"{auth_action} failed: {e}")

# Logout Button
if "user" in st.session_state and st.sidebar.button("Logout"):
    st.session_state.pop("user", None)
    supabase.auth.sign_out()
    st.rerun()

if "user" in st.session_state:
    user = st.session_state.get("user")
    uid = user.get("id") if user else None
    token = user.get("access_token") if user else None

    st.sidebar.success("‚úÖ Logged in successfully!")
    try:
        email = user.get("email", "Unknown")
        st.sidebar.markdown(f"**Logged in as:** {email}üë§")
    except Exception:
        st.sidebar.warning("‚ö†Ô∏è Failed to retrieve user email.")

    
    st.sidebar.subheader("üìÅ Your Groups")

    uid = st.session_state["user"].get("id")
    groups_resp = supabase.table("groups").select("*").eq("user_id", uid).execute()
    groups = groups_resp.data

    group_names = sorted([g["group_name"] for g in groups])
    group_lookup = {g["group_name"]: g for g in groups}

    with st.sidebar.expander("‚ûï Create New Group"):
                new_group_name = st.text_input("New Group Name")
                new_group_tickers = st.text_input("Tickers (comma-separated)", help="Example: AAPL, MSFT, TSLA")

                if st.button("Create Group"):
                    if new_group_name and new_group_tickers:
                        tickers_list = [t.strip().upper() for t in new_group_tickers.split(",") if t.strip()]
                        response = supabase.table("groups").insert({
                            "user_id": uid,
                            "group_name": new_group_name,
                            "tickers": tickers_list
                        }).execute()

                        if response.data:
                            st.session_state["last_saved_group"] = new_group_name
                            st.rerun()
                        else:
                            st.error(f"‚ùå Failed to save group. Response: {response}")
                    else:
                        st.warning("Please enter both a group name and ticker list.")
    
    with st.sidebar.expander("üì§ Upload Your Portfolio (CSV) or Use a Group"):
        st.markdown("CSV must include **Ticker** and **Shares** columns. Example:")
        st.code("Ticker,Shares\nAAPL,50\nTSLA,30")
        source_choice = st.radio("Choose Input Method", ["Upload CSV", "Use Group"], key="input_method_radio")

        tickers = []
        portfolio_weights = None

        if source_choice == "Upload CSV":
            portfolio_df, portfolio_weights, classifications = load_portfolio_from_csv()
            if portfolio_df is not None:
                tickers = portfolio_df["Ticker"].tolist()
                st.session_state["portfolio_classifications"] = classifications

        elif source_choice == "Use Group":
            selected_group = st.selectbox("Select Group", group_names)
            
            if selected_group:
                group_obj = group_lookup[selected_group]
                tickers = group_obj["tickers"]
                st.session_state["tickers"] = tickers

                if tickers:
                    st.markdown("### ‚öñÔ∏è Edit Weights (optional)")

                    # Provide a default weight dictionary (equal weight)
                    default_weights = {t: 1 / len(tickers) for t in tickers}

                    # Use Streamlit number inputs to allow editing weights
                    edited_weights = {}
                    for t in tickers:
                        edited_weights[t] = st.number_input(
                            label=f"Weight for {t}",
                            min_value=0.0,
                            max_value=1.0,
                            value=float(default_weights[t]),
                            step=0.01,
                            key=f"weight_input_{t}"
                        )

                    # Normalize weights to sum to 1
                    total = sum(edited_weights.values())

                    # ‚úÖ Move the check here, after total is defined:
                    if abs(total - 1.0) > 0.01:
                        st.warning("Weights will be normalized to sum to 100%.")

                    if total == 0:
                        st.warning("‚ö†Ô∏è Total weight is 0. Please adjust weights.")
                        portfolio_weights = None
                        st.session_state["portfolio_weights"] = None
                    else:
                        normalized_weights = {t: w / total for t, w in edited_weights.items()}
                        portfolio_weights = pd.Series(normalized_weights)
                        st.session_state["portfolio_weights"] = portfolio_weights

                        st.caption(f"üéØ Total Weight (before normalization): {total:.2f}")
                        st.dataframe(pd.Series(normalized_weights, name="Normalized Weight").round(4))
                    
                    # Pie chart of weights
                    st.markdown("üìä **Weight Allocation Pie Chart**")
                    pie_df = pd.DataFrame({
                        "Ticker": list(normalized_weights.keys()),
                        "Weight": list(normalized_weights.values())
                    })

                    fig = px.pie(pie_df, values="Weight", names="Ticker", hole=0.3)
                    st.plotly_chart(fig, use_container_width=True)

                    st.markdown("üìà **Weight Allocation Bar Chart**")
                    fig_bar = px.bar(pie_df, x="Ticker", y="Weight", text="Weight", title="Portfolio Weights")
                    fig_bar.update_traces(texttemplate='%{text:.2%}', textposition='outside')
                    fig_bar.update_layout(yaxis_tickformat=".0%", uniformtext_minsize=8, uniformtext_mode='hide')
                    st.plotly_chart(fig_bar, use_container_width=True)

                if group_obj["user_id"] == st.session_state["user"]["id"]:
                    with st.sidebar.expander("‚úèÔ∏è Edit/Delete Group"):
                        updated_tickers = st.text_input("Edit Tickers", ",".join(group_obj["tickers"]))
                        if st.button("Update Group"):
                            supabase.table("groups").update({
                                "tickers": [t.strip().upper() for t in updated_tickers.split(",")]
                            }).eq("id", group_obj["id"]).execute()
                            st.rerun()
                        if st.button("‚ùå Delete Group"):
                            supabase.table("groups").delete().eq("id", group_obj["id"]).execute()
                            st.rerun()

else:
    st.sidebar.info("Please log in to manage groups.")

# Date range
import datetime
today = datetime.date.today()
default_start = today - pd.DateOffset(years=5)
start = st.sidebar.date_input("Start Date", value=default_start.date(), min_value=datetime.date(2000, 1, 1), max_value=today)
end = st.sidebar.date_input("End Date", value=today, min_value=datetime.date(2000, 1, 1), max_value=today)
threshold_days = st.sidebar.slider(
    "‚öôÔ∏è Tolerance for start/end mismatch (days)",
    0, 10, 3,
    help="Recommended: 3 days to account for weekends, market holidays, market closures or data source lag. Set to 0 when using 'Daily' frequency for stricter matching."
)

# Return frequency and type
freq = st.sidebar.selectbox("Return Frequency", ["Daily", "Monthly", "Yearly"])
abs_or_pct = st.sidebar.radio("Return Type", ["% Change (Relative)", "Price (Absolute)"])

# Overlap logic for YoY
overlap_window = st.sidebar.selectbox("Overlap Windows (for Yearly)?", ["Yes", "No"])

# Correlation method
corr_type = st.sidebar.selectbox("Correlation Method", ["Pearson", "Kendall", "Spearman"])

# ---------------------------------------------
# Begin Analysis
# ---------------------------------------------
if st.sidebar.button("üîç Run Analysis"):
    with st.spinner("Fetching and analyzing data..."):
        data = {}
        incomplete_data_notes = []
        with st.status("üì• Downloading data...", expanded=True) as status:
            failed = []
            for ticker in tickers:
                stock = yf.download(ticker, start=start, end=end, group_by="column", auto_adjust=True)

                if not stock.empty:
                    # Check if the date range is incomplete
                    actual_start = stock.index.min().date()
                    actual_end = stock.index.max().date()
                    user_start = pd.to_datetime(start).date()
                    user_end = pd.to_datetime(end).date()

                    if (actual_start - user_start).days > threshold_days or (user_end - actual_end).days > threshold_days:
                        reason = "IPO, delisting, or missing Yahoo data"
                        incomplete_data_notes.append({
                            "Ticker": ticker,
                            "Available From": actual_start,
                            "Available To": actual_end,
                            "Requested From": user_start,
                            "Requested To": user_end,
                            "Reason": reason
                        })

                    # Extract price series
                    if isinstance(stock.columns, pd.MultiIndex):
                        try:
                            val = stock["Close"][ticker].dropna()
                        except KeyError:
                            st.warning(f"‚ö†Ô∏è 'Close' prices not found for {ticker}.")
                            failed.append(ticker)
                            continue
                    else:
                        if "Close" in stock.columns:
                            val = stock["Close"]
                        else:
                            st.error(f"{ticker} has no valid price columns.")
                            failed.append(ticker)
                            continue

                    if isinstance(val, pd.Series):
                        data[ticker] = val
                    elif isinstance(val, pd.DataFrame) and val.shape[1] == 1:
                        data[ticker] = val.iloc[:, 0]
                    else:
                        st.error(f"{ticker} has invalid format. Skipping.")
                        failed.append(ticker)
                else:
                    st.error(f"{ticker} returned no data.")
                    failed.append(ticker)
            status.update(label=f"‚úÖ Download complete. ({len(tickers) - len(failed)} success, {len(failed)} failed)", state="complete")

        if incomplete_data_notes:
            st.markdown("### ‚ö†Ô∏è Some stocks have limited data")
            with st.expander("Click to view details"):
                df_missing = pd.DataFrame(incomplete_data_notes)
                st.dataframe(df_missing)

        if len(data) == 0:
            st.error("‚ùå No valid data downloaded.")
        else:
            df = pd.DataFrame(data)
            df_norm = normalize_prices(df)
            returns = compute_returns(df, freq, abs_or_pct, overlap_window)
            st.write(f"üìä Data range in df: {df.index.min().date()} to {df.index.max().date()}")
            buffer_prices = io.StringIO()
            df.to_csv(buffer_prices)
            buffer_prices.seek(0)
            st.download_button("‚¨áÔ∏è Download Price Data CSV", data=buffer_prices.getvalue(), file_name="prices.csv", mime="text/csv")

            st.session_state["df"] = df
            st.session_state["returns"] = returns
            st.session_state["df_norm"] = df_norm
            st.session_state["tickers"] = returns.columns.tolist()
            st.session_state["start"] = start
            st.session_state["end"] = end
            st.session_state["portfolio_weights"] = portfolio_weights
            st.session_state["corr_type"] = corr_type
            st.session_state["analysis_complete"] = True

if st.session_state.get("analysis_complete"):
    render_results(
        df=st.session_state["df"],
        returns=st.session_state["returns"],
        df_norm=st.session_state["df_norm"],
        tickers=st.session_state["tickers"],
        start=st.session_state["start"],
        end=st.session_state["end"],
        portfolio_weights=st.session_state.get("portfolio_weights")
    )